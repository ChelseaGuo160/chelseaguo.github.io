[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chelseaguo.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I’m Chelsea, I’m a senior in MHC.\n\nHere’s the link to my page\nhttps://quarto.org/docs/websites"
  },
  {
    "objectID": "K-Means-Mini-Demo.html",
    "href": "K-Means-Mini-Demo.html",
    "title": "Lab: K-Means",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue.\n\n\n\n\nLoad Packages\nYou likely will need to install some these packages before you can run the code chunk below successfully.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(amap)\n\n\n\nLoad Penguin Data\n\ndata(penguins)\n\n\n\nData Cleaning\n\n# Remove missing values\n# \npenguins &lt;- penguins %&gt;%\n  filter(is.na(bill_length_mm)) & (is.na(bill_depth_mm) & is.na(species))\n\nError in eval(expr, envir, enclos): object 'bill_depth_mm' not found\n\n# Make data table (named penguins_reduced) that only has\n# bill_length_mm and bill_depth_mm columns\npenguins_reduced &lt;-penguins %&gt;% select(bill_length_mm)\n\n\n\nInitial Visualization\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() \n\n\n\n\n\n\n\n\nWe’ll cluster these penguins based on their bill lengths and depths:\n\n\nImplement \\(K\\)-Means\nComplete the code below to run the K-means algorithm using K = 3.\n\nset.seed(244)\n# Run the K-means algorithm\nkmeans_3_round_1 &lt;- kmeans(scale(penguins_reduced), centers = 3) \n\nError in do_one(nmeth): NA/NaN/Inf in foreign function call (arg 1)\n\n# Plot the cluster assignments\npenguins_reduced %&gt;% \n  mutate(kmeans_cluster = as.factor(kmeans_3_round_1$cluster)) %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = kmeans_cluster)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"K-means with K = 3 (round 1)\") + \n  theme_minimal()\n\nError in `mutate()`:\nℹ In argument: `kmeans_cluster = as.factor(kmeans_3_round_1$cluster)`.\nCaused by error in `is.factor()`:\n! object 'kmeans_3_round_1' not found\n\n\n\nWhy do we have to set the seed for K-means? In practice, why should we try out a variety of seeds?\n\nAnswer. YOUR ANSWER HERE\nThe optimal value in here appears to be k = 3\nFor reproducibility. Every time we run the function “kmeans”, it initializes the center of the clusters to be in random locations. It’s possible that same random locations give better clustering results than others; since k-mean is a greedy algorithm, it’s possible for it to have different results each time, and it’s possible for it to get “stuck” at a local solution.\n\n\nK-Means Clusters Versus Known Species Groupings\n\n  ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Actual Groupings of Data Based on Species\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nVisually, how well do you think \\(K\\)-means captured the underlying species structure of the data?\n\nAnswer. YOUR ANSWER HERE\nI think it\n\n\nTuning \\(K\\)\n\nTo implement K-means clustering we must choose an appropriate K! Use the following example to see the two different extreme situations. Typically, the ideal \\(K\\) is somewhere between the two extremes.\nMinimum: \\(K = 2\\) groups/clusters\nMaximum: \\(K = n\\) groups/clusters (one observation per cluster)\n\nWhat happens in the \\(K\\)-means algorithm if \\(K = n\\)?\nAnswer. YOUR ANSWER HERE\nLet’s consider anywhere from \\(K = 2\\) to \\(K = 20\\) clusters.\n\nset.seed(244)\n\nk_2  &lt;- kmeans(scale(penguins_reduced), centers = 2)\n\nError in do_one(nmeth): NA/NaN/Inf in foreign function call (arg 1)\n\nk_20 &lt;- kmeans(scale(penguins_reduced), centers = 20)\n\nError in do_one(nmeth): NA/NaN/Inf in foreign function call (arg 1)\n\npenguins_reduced %&gt;% \n  mutate(cluster_2 = as.factor(k_2$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_2)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 2\")\n\nError in `mutate()`:\nℹ In argument: `cluster_2 = as.factor(k_2$cluster)`.\nCaused by error in `is.factor()`:\n! object 'k_2' not found\n\npenguins_reduced %&gt;% \n  mutate(cluster_20 = as.factor(k_20$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_20)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 20\") + \n    scale_color_manual(values = rainbow(20))\n\nError in `mutate()`:\nℹ In argument: `cluster_20 = as.factor(k_20$cluster)`.\nCaused by error in `is.factor()`:\n! object 'k_20' not found\n\n\nWhat are your general impressions?\nAnswer. YOUR ANSWER HERE\n\n\nFinding Ideal K Value: Silhoutte\n\nThe average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\n\nTo do so, it maximizes the distance between clusters and minimizes distance within clusters.\n\nA high average silhouette indicates a good clustering.\nGiven a range of possible K values, the optimal number of clusters (K) is the one that maximizes the average silhouette.\n\nWe can use a built-in silhouette method in the fviz_nbclust function to compute the average silhouette for various K values.\n\nfviz_nbclust(scale(penguins_reduced), kmeans, method='silhouette')\n\nError in do_one(nmeth): NA/NaN/Inf in foreign function call (arg 1)\n\n\nBased on the average silhouette approach, what is the optimal \\(K\\) value?\nAnswer. YOUR ANSWER HERE\n\n\nExperimenting with Distance Metrics\nWe can use the Kmeans method (notice the “K” is capitalized in this function name) from the amap library to specify how we are measuring distance in the K-means algorithm.\n\nset.seed(244)\nk_2_manattan = Kmeans(scale(penguins_reduced), centers = 3, \n                      method = \"manhattan\")\n\nError in Kmeans(scale(penguins_reduced), centers = 3, method = \"manhattan\"): NA/NaN/Inf in foreign function call (arg 1)\n\nk_2_euclid = Kmeans(scale(penguins_reduced), centers = 3, \n                    method = \"euclidean\")\n\nError in Kmeans(scale(penguins_reduced), centers = 3, method = \"euclidean\"): NA/NaN/Inf in foreign function call (arg 1)\n\nk_2_maxnorm = Kmeans(scale(penguins_reduced), centers = 3, \n                     method = \"maximum\")\n\nError in Kmeans(scale(penguins_reduced), centers = 3, method = \"maximum\"): NA/NaN/Inf in foreign function call (arg 1)\n\nfviz_cluster(k_2_euclid, data = scale(penguins_reduced), \n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\nError in fviz_cluster(k_2_euclid, data = scale(penguins_reduced), main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", : object 'k_2_euclid' not found\n\nfviz_cluster(k_2_manattan, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\nError in fviz_cluster(k_2_manattan, data = scale(penguins_reduced), main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", : object 'k_2_manattan' not found\n\nfviz_cluster(k_2_maxnorm, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\nError in fviz_cluster(k_2_maxnorm, data = scale(penguins_reduced), main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", : object 'k_2_maxnorm' not found\n\n\nTry changing \\(K\\) to equal 3$ in the code chunk above. How do the clusterings using the 3 distance metrics compare? What do you generally observe?\nAnswer. YOUR ANSWER HERE\nModify the code in the chunk above so that we can easily change the value of K (rather than making sure to change K manually in every line). In general coding practices, is called extracting out a constant."
  },
  {
    "objectID": "Lab8_VariableSubsetSelection.html",
    "href": "Lab8_VariableSubsetSelection.html",
    "title": "Variable Subset Selection",
    "section": "",
    "text": "Best Subset Selection (Notes)\n\nWith \\(p\\) predictors, there are \\(2^p\\) possible models because there are \\(2^p\\) possible subsets of the \\(p\\) predictors. (Size of the powerset) – alternatively, two choices for each predictor, either include or not include\nIn best subset selection procedure, we fit all \\(2^p\\) models and choose the one with the best value of our chosen evaluation metric (e.g., test error estimate based on CV, using some dedicated test set, etc.). Our “evaluation metric” (equivalently, “error metric”) should fairly assess test performance of model (as opposed to training set performance)\n\\(2^p\\) models? This sounds expensive.\nExample: \\(p = 4\\), 16 potential models to fit (including the zero variable model, i.e., one with just an intercept)\nDoesn’t try to transform variables or include interaction terms – would have to manually include transformed variables or interaction models in model\n\n\n\nForward Stepwise Selection\n\nIdea: add variables one at a time, choosing the “best” variable each time\n\nThis is known as a greedy algorithm: goes for temporarily/locally best choice without thinking about some kind of long-term optimum\n\nHistorical note: many past (and current) implementations to determine which variable is “best” is to choose the one whose inclusion had the lowest p-value. Problematic: discussed at the end.\n\nBetter to use CV estimates of test error instead of choices based on p-values\n\n\n\n\nGeneral Notes\n\nBackwards and forwards selection can give different results due to greedy behavior of each\nWhen several “reasonable” methods (e.g., forward/backward selection if best subset is prohibitively expensive), best practice is to try all methods, compare their results, and report all results\n\n\n\nCautions\n\nSome machine learning practitioners don’t like automated selection methods, b/c encourage us to not think about the variables b/c can just dump data into an algorithm and get a model\nNeed to think of variables in context, and carelessly using automated procedures can do real harm\nUsing p-values for deciding which variables to add/remove results gives unstable (and often “undesireable”) results when several quantitative predictors are correlated with one another (called collinearity – e.g., one predictor is a linear combination of others) – not the same as a violated independence assumption\n\nStatistical inference problems on final selected model: chosen through multiple hypothesis testing * Because we come to select a final model by trying a large number of models, multiple hypothesis testing is a big issue.\n\nWith multiple testing, the idea is that testing many hypotheses runs the risk of a result being statistically significant just by chance.\nMight have to do a Bonferroni correction or something like that (ignore this comment if you haven’t taken STAT 242)\n\nCIs are misleadingly narrow (from multiple hyp. testing)\nP-values are misleadingly small (from mult. hyp testing)\n\n\n\n\nSummary\nSubset selection methods use model quality metrics to search through different models in an automated way\n\nBest subset selection: intuitive, accurate, quickly computationally expensive\nStepwise selection (either forward or backwards): faster but not guaranteed to find “best model” due to greedy nature\nNo methods automatically consider transformations or interactions (we manually include those to exlore them)\nProblems with statistical inference\n\nShrinkage/regularization methods are a popular alternative to subset selection.\n\n\nFor Fun: Heat Map for Exploring Multicollinearity\n\n# Load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\n# Load data\nhealth_data = read_xlsx(\"healthdata.xlsx\")\nhead(health_data)\n\n# A tibble: 6 × 13\n    age weight  neck chest abdomen   hip thigh  knee ankle biceps forearm wrist\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    42   136.  37.8  87.6    77.6  88.6  51.9  34.9  22.5   27.7    27.5  18.5\n2    61   143   36.5  93.4    83.3  93    55.5  35.2  20.9   29.4    27    16.8\n3    49   213.  40.8 105.    107.  108.   66.5  42.5  24.5   35.5    29.8  18.7\n4    40   168.  34.2  97.8    92.3 101.   57.5  36.8  22.8   32.1    26    17.3\n5    52   199.  39.4 107.    100   105    63.9  39.2  22.9   35.7    30.4  19.2\n6    55   125   33.2  87.7    76    88.6  50.9  35.4  19.1   29.3    25.7  16.9\n# … with 1 more variable: height &lt;dbl&gt;\n\n\n\n# Get the correlation matrix\nlibrary(reshape2)\ncor_matrix &lt;- cor(health_data)\ncor_matrix[lower.tri(cor_matrix)] &lt;- NA\ncor_matrix &lt;- cor_matrix %&gt;% \n  melt() %&gt;% \n  na.omit() %&gt;% \n  rename(correlation = value)\n\n# Visualize the correlation for each pair of variables\nggplot(cor_matrix, aes(x = Var1, y = Var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\", \n    midpoint = 0, limit = c(-1,1)) +\n  labs(x = \"\", y = \"\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n# STEP 1: Model Specification\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\n# STEP 2: Model estimation\nheight_model_1 &lt;- lm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\n# Look at model\nheight_model_1 %&gt;% tidy()\n\n# A tibble: 13 × 5\n   term        estimate std.error statistic p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept) 125.        61.9       2.02   0.0531\n 2 age          -0.112      0.132    -0.847  0.405 \n 3 weight        0.379      0.213     1.78   0.0864\n 4 neck          0.139      1.17      0.119  0.906 \n 5 chest        -0.459      0.473    -0.971  0.340 \n 6 abdomen       0.283      0.354     0.798  0.432 \n 7 hip          -0.921      0.510    -1.81   0.0822\n 8 thigh        -1.24       0.646    -1.92   0.0660\n 9 knee          0.151      0.941     0.160  0.874 \n10 ankle        -0.888      1.28     -0.693  0.494 \n11 biceps       -0.0808     0.746    -0.108  0.915 \n12 forearm       2.25       1.80      1.25   0.223 \n13 wrist         0.836      2.32      0.361  0.721 \n\n\n\n\nBest Subset Selection Algorithm\n\nBuild all possible models that use any combination of the available predictors\nIdentify the best model with respect to some chosen metric (eg: CV MAE, CV MSE) and context.\n\n\n\nExercise\nSuppose we used this algorithm for our height model with all 12 possible predictors (13 variables total, one is height, leaving 12 predictors). What’s the main drawback?\n\nThere are 2^12 models to fit, and it’s conmupationally expensive.\n\n\n\nBackward Stepwise Selection Algorithm\n\nBuild a model with all \\(p\\) possible predictors,\nRepeat the following until only 1 predictor remains in the model:\n\nRemove the 1 predictor that increases the MSE/MAE by the least\nBuild a model with the remaining predictors.\n\n\nYou now have \\(p\\) competing models: one with all \\(p\\) predictors, one with \\(p-1\\) predictors, …, and one with 1 predictor. In a future HW assignment, you will implement the above using MSE/MAE error metrics.\nFor this example though, for simplicity, we will identify the “best” model with respect to p-values.\nLet’s try out the first few steps!\nFirst, we would compute the 10-fold MAE of the model with all 12 predictors. The model with 12 predictors has a MAE of about 5.555\n\nset.seed(244)\nmodel_12_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nmodel_12_predictors_cv %&gt;%  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    5.56    10   0.840 Preprocessor1_Model1\n\n\nThen let’s look at which predictor to remove to identify the “best” model with 11 predictors.\n\n# Original model with 12 predictors\n# Find the \"least significant\" predictor: we take out biceps\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n# A tibble: 12 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age      -0.112      0.132    -0.847  0.405 \n 2 weight    0.379      0.213     1.78   0.0864\n 3 neck      0.139      1.17      0.119  0.906 \n 4 chest    -0.459      0.473    -0.971  0.340 \n 5 abdomen   0.283      0.354     0.798  0.432 \n 6 hip      -0.921      0.510    -1.81   0.0822\n 7 thigh    -1.24       0.646    -1.92   0.066 \n 8 knee      0.151      0.941     0.160  0.874 \n 9 ankle    -0.888      1.28     -0.693  0.494 \n10 biceps   -0.0808     0.746    -0.108  0.915 \n11 forearm   2.25       1.80      1.25   0.223 \n12 wrist     0.836      2.32      0.361  0.721 \n\n\nLooks like we should remove “biceps” as a predictot, because it appears to be the least statistically significance\nWe would then compute the 10-fold MAE of the model with 11 predictors.\n\n# 11 predictors\nlm_spec %&gt;% \n  fit(height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n# A tibble: 11 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age       -0.111     0.130    -0.858  0.398 \n 2 weight     0.369     0.190     1.94   0.0623\n 3 neck       0.161     1.14      0.142  0.888 \n 4 chest     -0.453     0.461    -0.982  0.334 \n 5 abdomen    0.281     0.348     0.809  0.425 \n 6 hip       -0.902     0.470    -1.92   0.0652\n 7 thigh     -1.26      0.602    -2.09   0.0454\n 8 knee       0.180     0.886     0.203  0.841 \n 9 ankle     -0.878     1.26     -0.699  0.490 \n10 forearm    2.17      1.62      1.34   0.192 \n11 wrist      0.907     2.18      0.416  0.681 \n\nset.seed(244)\nmodel_11_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + neck + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nmodel_11_predictors_cv %&gt;%  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    5.55    10   0.911 Preprocessor1_Model1\n\n\nLooks like we should remove…. It looks like the 10 predictor model with neck removed has MAE 5.552…\n\n# 10 predictors: the MAE of the \"best\" model with 11 predictors(biceps was removed) is 5.553447 \n# UPDATE this code: We can remove \"neck\" since is the lease statistical significant  \nlm_spec %&gt;% \n  fit(height ~ age + weight + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n# A tibble: 10 × 5\n   term    estimate std.error statistic p.value\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age       -0.111     0.127    -0.869  0.392 \n 2 weight     0.377     0.179     2.11   0.0435\n 3 chest     -0.460     0.451    -1.02   0.316 \n 4 abdomen    0.298     0.322     0.924  0.363 \n 5 hip       -0.931     0.416    -2.24   0.0331\n 6 thigh     -1.26      0.591    -2.14   0.0409\n 7 knee       0.166     0.866     0.191  0.850 \n 8 ankle     -0.884     1.23     -0.716  0.480 \n 9 forearm    2.29      1.37      1.66   0.107 \n10 wrist      0.985     2.07      0.475  0.639 \n\nset.seed(244)\nmodel_10_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + chest + abdomen + hip + thigh + knee + ankle + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nmodel_10_predictors_cv %&gt;%  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    5.51    10   0.903 Preprocessor1_Model1\n\n\n\n# 9 predictors: find the new predictor to remove, knee is removed\nlm_spec %&gt;% \n  fit(height ~ age + weight + chest + abdomen + hip + thigh + ankle + forearm + wrist,\n      data = health_data) %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(p.value = round(p.value, 4))\n\n# A tibble: 9 × 5\n  term    estimate std.error statistic p.value\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 age       -0.101     0.115    -0.877  0.387 \n2 weight     0.397     0.143     2.78   0.0093\n3 chest     -0.496     0.404    -1.23   0.229 \n4 abdomen    0.293     0.316     0.927  0.361 \n5 hip       -0.926     0.409    -2.27   0.0308\n6 thigh     -1.25      0.575    -2.17   0.0381\n7 ankle     -0.898     1.21     -0.742  0.464 \n8 forearm    2.32      1.34      1.73   0.0931\n9 wrist      0.877     1.96      0.446  0.658 \n\nset.seed(244)\nmodel_9_predictors_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n   height ~ age + weight + chest + abdomen + hip + thigh + ankle + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\nmodel_9_predictors_cv %&gt;%  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    5.43    10   0.913 Preprocessor1_Model1\n\n\n\n\nBackward Stepwise Selection Step-by-Step Results\nBelow is the complete model sequence along with 10-fold CV MAE for each model (using set.seed(244)).\n\n\n\npred\nCV MAE\npredictor list\n\n\n\n\n12\n5.555\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck, biceps\n\n\n11\n5.553\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee, neck\n\n\n10\n5.512\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist, knee\n\n\n9\n5.368\nweight, hip, forearm, thigh, chest, abdomen, age, ankle, wrist\n\n\n8\n5.047\nweight, hip, forearm, thigh, chest, abdomen, age, ankle\n\n\n7\n5.013\nweight, hip, forearm, thigh, chest, abdomen, age\n\n\n6\n4.684\nweight, hip, forearm, thigh, chest, abdomen\n\n\n5\n4.460\nweight, hip, forearm, thigh, chest\n\n\n4\n4.386\nweight, hip, forearm, thigh\n\n\n3\n4.091\nweight, hip, forearm\n\n\n2\n3.733\nweight, hip\n\n\n1\n3.658\nweight"
  },
  {
    "objectID": "Lab10_Lasso_Cont.html",
    "href": "Lab10_Lasso_Cont.html",
    "title": "Lab 10: LASSO (Continued)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "href": "Lab10_Lasso_Cont.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Build the Model for a Range of Tuning Parameter Values",
    "text": "Build the Model for a Range of Tuning Parameter Values\n\n# STEP 1: LASSO Model Specification\nlasso_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\nSTEP 1 Notes:\n\nWe use the glmnet, not lm, engine to build the LASSO.\nThe glmnet engine requires us to specify some arguments (set_args):\n\nmixture = 1 indicates LASSO. Changing this would run a different regularization algorithm.\npenalty = tune() indicates that we don’t (yet) know an appropriate \\(\\lambda\\) penalty term. We need to tune it.\n\n\nSuppose we want to build a model of response variable y using all possible predictors in a data frame sample_data.\n\n# STEP 2: Variable Recipe\nvariable_recipe &lt;- recipe(y ~ height., data = health_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nSTEP 2 Notes:\n\ny ~ . is shorthand for “y as a function of all other variables”\nThe function step_dummy() turns all potentially categorical (sometimes called nominal) predictors into indicator variables (which are unfortunately called “dummy variables” in some circles, hence the terrible name)\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nSTEP 3 Notes:\n\nThe function add_recipe includes our variable_recipe created in Step 2, which is specifying what predictors/response variables we have and what data set those variables belong to\nThe function add_model includes our lasso_spec created in Step 1, which is specifying what kind of model we wish to use (in this case, regression with LASSO)\n\n\n# STEP 4: Estimate Multiple LASSO Models Using a Range of Possible Lambda Values\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(___, ___)), levels = ___),\n    resamples = vfold_cv(sample_data, v = ___),\n    metrics = metric_set(mae)\n  )\n\nSTEP 4 Notes:\n\nSince the CV process is random, we need to set.seed(___).\nWe use tune_grid() instead of fit() since we have to build multiple LASSO models, each using a different tuning parameter.\nThe function grid specifies the values of tuning parameter \\(\\lambda\\) that we want to try.\n\npenalty(range = c(___, ___)) specifies a range of \\(\\lambda\\) values we want to try, on the log10 scale.\nYou might start with c(-5, 1), which uses the range \\(\\lambda\\) from 0.00001 (\\(10^(-5)\\)) to 10 (\\(10^1\\)), and adjust from there.\nThe function levels is the number of \\(\\lambda\\) values to try in that range, thus how many LASSO models to build.\n\nThe functionsresamples and metrics indicate that we want to calculate a CV MAE (since mae is in metric_set) for each LASSO model. The number of folds in our cross-validation is given by v."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#tuning-lambda",
    "href": "Lab10_Lasso_Cont.html#tuning-lambda",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Tuning \\(\\lambda\\)",
    "text": "Tuning \\(\\lambda\\)\nThe ### Plotting CV MAE (Y-Axis) for the LASSO Model for Each \\(\\lambda\\) (X-axis)\n\n# Calculate CV MAE for each LASSO model\nlasso_models %&gt;% collect_metrics()\n\n# Plotting option 1: plot lambda on log10 scale\nautoplot(lasso_models) + scale_x_log10()\n\n# Plotting option 2: plot lambda on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() +\n  xlab(expression(lambda))\n\n# Plotting option 3: CV MAE (y-axis) with error bars (+/- 1 standard error)\n# with lambda (x-axis) on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n# Identify lambda which produced the lowest (\"best\") CV MAE\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# Identify the largest lambda for which the CV MAE is\n# larger but \"roughly as good\" (within one standard error of the lowest)\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n\nFinalizing the “Best” LASSO Model\nIt seems the best lambda is one that smallest CV MAE, which we found as 0.034. The biggest lambda that is within 1 standard error of this “best” lambda(i.e. is within the error bar of the “best” lambda)\n\n# Parameters = final lambda value (best_penalty or parsimonious_penalty)\nfinal_lasso_model &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = sample_data)\n\n# Check it out\nfinal_lasso_model %&gt;% tidy()\n\n\n\nUsing Final LASSO Model to Make Predictions\n\nfinal_lasso_model %&gt;% \n  predict(new_data = SOME DATA FRAME W/ OBSERVATIONS ON EACH PREDICTOR)\n\n\n\nVisualizing Shrinkage\nThis code can help us visualize shrinkage by comparing LASSO coefficients under each \\(\\lambda\\).\n\n# Get output for each LASSO\nall_lassos &lt;- final_lasso_model %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n\n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n# For example, what are variables 2 and 4?\nrownames(all_lassos$beta)[c(2,4)]"
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#context",
    "href": "Lab10_Lasso_Cont.html#context",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Context",
    "text": "Context\n\nWorld = supervised learning\nWe want to model some output variable \\(Y\\) using a set of potential predictors (\\(X_1, X_2, ..., X_p\\)).\nTask = regression\n\\(y\\) is quantitative\nModel = linear regression\nWe’ll assume that the relationship between \\(Y\\) and (\\(X_1, X_2, ..., X_p\\)) can be represented by the model equation\n\\[\\mathbb{E}(Y \\mid X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nand corresponding regression equation\n\\[\\hat{Y} = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\cdots + \\hat\\beta_p X_p \\]\nEstimation algorithm = LASSO"
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "href": "Lab10_Lasso_Cont.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "title": "Lab 10: LASSO (Continued)",
    "section": "LASSO: Least Absolute Shrinkage and Selection Operator",
    "text": "LASSO: Least Absolute Shrinkage and Selection Operator\nIdea\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient). Then track whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.\nCriterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[SSR + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nsum of squared residuals (SSR) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter\n\nPicking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the “best” values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)\n\nWe will explore what this means in more detail in one of the exercises."
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#exercises",
    "href": "Lab10_Lasso_Cont.html#exercises",
    "title": "Lab 10: LASSO (Continued)",
    "section": "Exercises",
    "text": "Exercises\nWe will use the LASSO algorithm to help us build a good predictive model of height using the collection of 12 possible predictors in the new_health_data data set:\n\n\nError: 'health_data_updated.csv' does not exist in current working directory ('/home/guo32j/Stat244/chelseaguo.github.io').\n\n\nLet’s implement the LASSO. We’ll pause and adjust the code from the R Code Notes for LASSO section.\n\n# STEP 1: LASSO Algorithm & Model Specification\n# This is copied exactly from the R Code Notes for LASSO Section\nlasso_spec &lt;- linear_reg() %&gt;%             \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;%                 \n  set_args(mixture = 1, penalty = tune()) \n\n\n# STEP 2: Variable Recipe\n# y ~. for response y \nvariable_recipe &lt;- recipe(height ~ ., data = new_health_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nError in rlang::is_missing(data): object 'new_health_data' not found\n\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\n# This is copied exactly from the R Code Notes for LASSO Section\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nError in is_recipe(recipe): object 'variable_recipe' not found\n\n\n\n# STEP 4: Estimate 50 LASSO models using \n# lambda values on a \"grid\" or range from 10^(-5) to 10^(-0.1).\n# Calculate the 10-fold CV MAE for each of the 50 models.\n# Note: I usually start with a range from 10^(-5) to 10^1 and \n# tweak through trial-and-error.\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, .1)), levels = 50),\n    resamples = vfold_cv(new_health_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nError in tune_grid(., grid = grid_regular(penalty(range = c(-5, 0.1)), : object 'lasso_workflow' not found\n\n\nLast time: The code below finds the “best” \\(\\lambda\\) (the \\(\\lambda\\) for which the CV MAE is smallest).\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\n\nError in select_best(., metric = \"mae\"): object 'lasso_models' not found\n\nbest_penalty\n\nError in eval(expr, envir, enclos): object 'best_penalty' not found\n\n\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\nError in autoplot(lasso_models): object 'lasso_models' not found\n\n\n\nParsimonious Model\nSuppose we prefer a parsimonious model.\nThe “parsimonious model” is the one with the fewest necessary variables while still maintaining predictive accuracy.\nIn our case, we will define it as the model with the largest possible \\(\\lambda\\) (i.e., biggest penalty for adding new variables) that still has a CV MAE within 1 standard error of the “best” model (the model whose \\(\\lambda\\) gives the lowest CV MAE).\nThe plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err),\n                alpha = 0.5)\n\nError in autoplot(lasso_models): object 'lasso_models' not found\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\nUse the plot with error bars to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse).\n\nANSWER.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\n\nError in select_by_one_std_err(., metric = \"mae\", desc(penalty)): object 'lasso_models' not found\n\nparsimonious_penalty\n\nError in eval(expr, envir, enclos): object 'parsimonious_penalty' not found\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here.\n\nANSWER.\n\n\n\n\n\nPicking a Range to Try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. (If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\n\nThe “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\n\nOur range was narrow enough.\n\nWe didn’t observe any loooooong flat lines in CV MAE. Thus, we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nModify your previous code to start with \\(\\lambda\\) in the range 10^(-5) to 10^(1) and see what you observe. Adjust the range until it seems appropriate (no flat lines, but no minimum on a boundary).\n\nANSWER When starting with lambda between 10^-5 and 10 ^15 there is a long flat line for the larger lambda values,indicating that maybe we picked a range that was too wide. For comupational efficency, we can try a smaller range of lambda values. For me, picking lambda b/w 10^-5 and 10^-1 seemed good enough\n\n\n\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = new_health_data)\n\nError in finalize_workflow(., parameters = parsimonious_penalty): object 'lasso_workflow' not found\n\nfinal_lasso %&gt;% \n  tidy()\n\nError in tidy(.): object 'final_lasso' not found\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\nLet’s finalize our LASSO model.\n\nHow many and which predictors were kept in this model?\n\n\nANSWER. 7 seems good enough\n\n\nThrough shrinkage, the LASSO coefficients(the \\(\\hat\\beta_i\\)) lose some contextual meaning, so we typically shouldn’t interpret them. Why? THINK: What is the goal of LASSO modeling?\n\n\nANSWER. In least suqares the beta coefficients have specific meanings(eg. beta1 represents the change in mean height for every year increase in age, holding all other predictors fixed). Now, the estimates of the coefficients from LASSO don’t have the same meaning, b/c they no longer come from minimizing the SSR(as iss the case for least squares). However, that is okay, b/c we are using LASSO to find the specific values/estimates of the best coefficients, we’re using LASSO to find which predictors in regular least square, and then use those beta estimates(from least squares)\n\n\n\nOur parsimonious LASSO selected only 7 of the 12 possible predictors. Out of curiosity, how many predictors would have remained if we had used the best_penalty value for \\(\\lambda\\)?\n\nlasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = new_health_data) %&gt;% \n  tidy() %&gt;% \n  filter(estimate != 0)\n\nError: &lt;text&gt;:2:35: unexpected input\n1: lasso_workflow %&gt;% \n2:   finalize_workflow(parameters = __\n                                     ^\n\n\n\nANSWER. When we use the “best” lambda instead of the parsimonious one, we are left w/ keeping all 11 predictors, which feels like a lot. This is why researchers typically prefer the parsimonious values of the tunning parameter.\n\nBased on this example, do you think LASSO is a greedy algorithm? Are you “stuck” with your past locally optimal choices? Compare the predictors in this larger model with those in the smaller, parsimonious model.\n\nANSWER. LASSO is not greedy. We would mot be stuck with selecting “chest” as a prefictor for all future models; we would be able to drop it. If we were to run backwards stwpwise selection(this isn’t obvious – Laura just tellin’ ya), as some point, including “chest” would have been locally optimal, so we would have been stuck with it in our greedy algorithm. Take away: LASSO is not greedy and it is more stable than variable selection algorithm, especially those that rely on p-values"
  },
  {
    "objectID": "Lab10_Lasso_Cont.html#lasso-vs-least-squares",
    "href": "Lab10_Lasso_Cont.html#lasso-vs-least-squares",
    "title": "Lab 10: LASSO (Continued)",
    "section": "LASSO vs Least Squares",
    "text": "LASSO vs Least Squares\nLet’s compare our final_lasso model to the least squares model using all predictors.\n\n# Build the LS model\nlm_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lm\")\n\nls_workflow &lt;- workflow() %&gt;%\n  add_model(lm_spec) %&gt;%\n  add_recipe(variable_recipe)\n\nError in is_recipe(recipe): object 'variable_recipe' not found\n\nls_model &lt;- ls_workflow %&gt;% \n  fit(data = new_health_data) \n\nError in fit(., data = new_health_data): object 'ls_workflow' not found\n\n# examine coefficients\nls_model %&gt;% \n  tidy()\n\nError in tidy(.): object 'ls_model' not found\n\n# get 10-fold CV MAE\nset.seed(123)\nls_workflow %&gt;% \n  fit_resamples(\n    resamples = vfold_cv(new_health_data,v = 10),\n    metrics = metric_set(mae)\n  ) %&gt;% \n  collect_metrics()\n\nError in fit_resamples(., resamples = vfold_cv(new_health_data, v = 10), : object 'ls_workflow' not found\n\n\n\nOur final_lasso has ___ predictors and a CV MAE of ___ (calculated above). The ls_model has 12 predictors and a CV MAE of ___ Comment.\n\n\nANSWER\n\n\nUse both final_lasso and ls_model to predict the height of the new patient below. How do these compare? Does this add to or calm any fears you might have had about shrinking coefficients?!\n\n\nnew_patient &lt;- data.frame(age = 50, weight = 200, neck = 40, \n                          chest = 115, abdomen = 105, hip = 100, \n                          thigh = 60, knee = 38, ankle = 23, biceps = 32, \n                          forearm = 29, wrist = 19) \n\n\n# LS prediction\n___ %&gt;% \n  predict(new_data = new_patient)\n\n\n# LASSO prediction\n___ %&gt;% \n  predict(new_data = new_patient)\n\nUsing the LASSO model with only 7 predictors, we have a predicted height of70.15 inches.\n\nWhich final model would you choose, the LASSO or least squares?\n\n\nANSWER. LASSO ideally should havr some prediction within range(on average) of those using least squares without being overfit"
  },
  {
    "objectID": "Intro-CV.html",
    "href": "Intro-CV.html",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Intro-CV.html#context",
    "href": "Intro-CV.html#context",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Context",
    "text": "Context\n\nBroader subject area: supervised learning\nWe want to build a model for some output RV \\(Y\\) given various predictor RVs \\(X_1, \\ldots, X_p\\).\nTask: regression\n\\(Y\\) is quantitative (takes numerical values)\nAlgorithm: linear regression model\nWe’ll assume that the relationship between \\(Y\\) and \\(X\\) can be represented by\n\n\\[\\mathbb{E}(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p  \\]"
  },
  {
    "objectID": "Intro-CV.html#review-k-fold-cross-validation",
    "href": "Intro-CV.html#review-k-fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Review: \\(k\\)-Fold Cross Validation",
    "text": "Review: \\(k\\)-Fold Cross Validation\nWe can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\n\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\n\nFit a model using the data in the other \\(k-1\\) folds (training).\n\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\n\nCalculate the MAE for fold \\(j\\) (testing): \\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\).\n\nCombine this information into one measure of model quality: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]\n\n\n\n\n\n\n\n\nVocabulary\n\n\n\n\n\n\nA tuning parameter is parameter or quantity upon which an algorithm depends whose value is selected or tuned to “optimize” the algorithm.\n\nFor \\(k\\)-fold CV, the tuning parameter is \\(k\\). // the smallest value of 2 and the maximum is the number of the observations\n//k is usually picked to be something in the middle"
  },
  {
    "objectID": "Intro-CV.html#set-up",
    "href": "Intro-CV.html#set-up",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Set Up",
    "text": "Set Up\n\nLoad Libraries\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\n# Load data\ndata(trees)\n\n\n\nRename Columns (Variables)\n\nRename Girth to diameter\nRename Height to height\n\n\n# Rename columns\ntrees = trees %&gt;% \n  rename(diameter = Girth, height = Height) %&gt;%\n  # Only have height and diameter be the columns \n  select(height, diameter)\n\n\n\nVisualization (Scatter Plot)\n\n# Create a scatter plot\nggplot(trees, aes(x = diameter, y = height)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nHeight appears to be (weakly) positively correlated with diameter\n\n\n\n\n\n# Step 1: Model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  # Output Y is quantitative\n  set_mode(\"regression\") %&gt;%\n  # Want regression to be lienar\n  set_engine(\"lm\")\n\n\n# Step 2: Model estimation\ntree_model &lt;- lm_spec %&gt;% fit(height ~ diameter, data = trees)"
  },
  {
    "objectID": "Intro-CV.html#fold-cross-validation",
    "href": "Intro-CV.html#fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "10-Fold Cross Validation",
    "text": "10-Fold Cross Validation\n\nProcedure\n\nRandomly split the data into 10 folds\nBuilt model 10 times, leaving 1 test fold out each time\nEvaluate each model on the test fold (using MAE/MSE and R-squared as error metrics)\n\n\n# For reproducibility\nset.seed(242)\n\ntree_model_cv = lm_spec %&gt;% \n# fit_resamples() function is for fitting on folds\nfit_resamples(\n  # Specify the relationship\n  height ~ diameter, \n  # vfold_cv makes CV folds randomly from\n  # trees data set\n  resamples = vfold_cv(trees, v = 10), \n  # Specify the error metrics\n  # (MAE, square root MSE, R^2)\n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ntree_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   4.38     10   0.639 Preprocessor1_Model1\n2 rmse    standard   5.17     10   0.733 Preprocessor1_Model1\n3 rsq     standard   0.341    10   0.121 Preprocessor1_Model1\n\n\n\n\nSummarizing\n\n# Get fold-by-fold results\n# Get info for each test fold \ntree_model_cv %&gt;% unnest(.metrics) %&gt;%  \nfilter(.metric == \"mae\") \n\n# A tibble: 10 × 7\n   splits         id     .metric .estimator .estimate .config           .notes  \n   &lt;list&gt;         &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n 1 &lt;split [27/4]&gt; Fold01 mae     standard        4.23 Preprocessor1_Mo… &lt;tibble&gt;\n 2 &lt;split [28/3]&gt; Fold02 mae     standard        5.67 Preprocessor1_Mo… &lt;tibble&gt;\n 3 &lt;split [28/3]&gt; Fold03 mae     standard        7.92 Preprocessor1_Mo… &lt;tibble&gt;\n 4 &lt;split [28/3]&gt; Fold04 mae     standard        3.59 Preprocessor1_Mo… &lt;tibble&gt;\n 5 &lt;split [28/3]&gt; Fold05 mae     standard        6.11 Preprocessor1_Mo… &lt;tibble&gt;\n 6 &lt;split [28/3]&gt; Fold06 mae     standard        1.42 Preprocessor1_Mo… &lt;tibble&gt;\n 7 &lt;split [28/3]&gt; Fold07 mae     standard        1.99 Preprocessor1_Mo… &lt;tibble&gt;\n 8 &lt;split [28/3]&gt; Fold08 mae     standard        2.76 Preprocessor1_Mo… &lt;tibble&gt;\n 9 &lt;split [28/3]&gt; Fold09 mae     standard        5.70 Preprocessor1_Mo… &lt;tibble&gt;\n10 &lt;split [28/3]&gt; Fold10 mae     standard        4.37 Preprocessor1_Mo… &lt;tibble&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 6(the smallest) and worst for fold 3(the biggest)"
  },
  {
    "objectID": "Intro-CV.html#exercise-1-in-sample-metrics",
    "href": "Intro-CV.html#exercise-1-in-sample-metrics",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 1: In-Sample Metrics",
    "text": "EXERCISE 1: In-Sample Metrics\nUse the health_data data to build two separate models of height:\n\n# STEP 2: model estimation\nmodel_1 &lt;-lm_spec %&gt;% \n  fit(height ~ hip + weight + thigh + knee + ankle, data = health_data)\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight * + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\nCalculate the in-sample R-squared for both models:\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.366         0.272  6.13      3.92 0.00650     5  -126.  266.  278.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.669         0.242  6.26      1.57   0.175    22  -113.  274.  315.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe R^2 value for the first model is about 0.366, and fir the second model is 0.526\nCalculate the in-sample MAE for both models:\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        3.48\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        2.93"
  },
  {
    "objectID": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "href": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 2: In-Sample Model Comparison",
    "text": "EXERCISE 2: In-Sample Model Comparison\nWhich model seems “better” by the in-sample metrics you calculated above? Any concerns about either of these models?\n\nBased on the in-samole MAE(ie, the MAE of the same data used to build/train the model), it appears that model 2 its is about 3.366 is better than model 1, 3.481.\n\nThe concern is that we are using the dame data that we built the model with to evaluate the model’s error/performence, which overly-specific to the data used to build"
  },
  {
    "objectID": "Intro-CV.html#exercise-3-10-fold-cv",
    "href": "Intro-CV.html#exercise-3-10-fold-cv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 3: 10-Fold CV",
    "text": "EXERCISE 3: 10-Fold CV\nComplete the code to run 10-fold cross-validation for our two models.\nmodel_1: height ~ hip + weight + thigh + knee + ankle\nmodel_2: height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist\n\n# 10-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height  ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height  ~ chest * age * weight + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae)\n  )"
  },
  {
    "objectID": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "href": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 4: Calculating the CV MAE",
    "text": "EXERCISE 4: Calculating the CV MAE\n\nUse collect_metrics() to obtain the cross-validated MAE and \\(R^2\\) for both models.\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    4.13    10   0.897 Preprocessor1_Model1\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    6.28    10   0.921 Preprocessor1_Model1\n\n\n\nInterpret the cross-validated MAE and \\(R^2\\) for model_1.\n\n\nWe expect our first model to produce predictions of height that are roughly off by 4.13 the observed MAE on average. For the first model, we expected to be roughly 0.28 of the variability based on the R^2 value in the i=observed heights of patients in the data set."
  },
  {
    "objectID": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "href": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 5: Fold-By-Fold Results",
    "text": "EXERCISE 5: Fold-By-Fold Results\nThe command collect_metrics() gave the final CV MAE, or the average MAE across all 10 test folds. The command unnest(.metrics) provides the MAE from each test fold.\n\nObtain the fold-by-fold results for the model_1 cross-validation procedure using unnest(.metrics).\n\n\nmodel_1_cv %&gt;% \n  unnest(.metrics)%&gt;%\n  filter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits         id     .metric .estimator .estimate .config           .notes  \n   &lt;list&gt;         &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n 1 &lt;split [36/4]&gt; Fold01 mae     standard       3.34  Preprocessor1_Mo… &lt;tibble&gt;\n 2 &lt;split [36/4]&gt; Fold02 mae     standard       0.820 Preprocessor1_Mo… &lt;tibble&gt;\n 3 &lt;split [36/4]&gt; Fold03 mae     standard       2.11  Preprocessor1_Mo… &lt;tibble&gt;\n 4 &lt;split [36/4]&gt; Fold04 mae     standard       5.16  Preprocessor1_Mo… &lt;tibble&gt;\n 5 &lt;split [36/4]&gt; Fold05 mae     standard      10.9   Preprocessor1_Mo… &lt;tibble&gt;\n 6 &lt;split [36/4]&gt; Fold06 mae     standard       2.25  Preprocessor1_Mo… &lt;tibble&gt;\n 7 &lt;split [36/4]&gt; Fold07 mae     standard       4.11  Preprocessor1_Mo… &lt;tibble&gt;\n 8 &lt;split [36/4]&gt; Fold08 mae     standard       2.91  Preprocessor1_Mo… &lt;tibble&gt;\n 9 &lt;split [36/4]&gt; Fold09 mae     standard       6.18  Preprocessor1_Mo… &lt;tibble&gt;\n10 &lt;split [36/4]&gt; Fold10 mae     standard       3.52  Preprocessor1_Mo… &lt;tibble&gt;\n\n\n\nWhich fold had the worst average prediction error and what was it?\n\n\nThe biggest one.\n\n\nRecall that collect_metrics() reported a final CV MAE of 4.13 for model_1. Confirm this calculation by wrangling the fold-by-fold results from part a.\n\n\n# Code here\nmodel_1_cv %&gt;% \n  unnest(.metrics)%&gt;%\n  filter(.metric == \"mae\") %&gt;%\n  summarize(mean(.estimate))\n\n# A tibble: 1 × 1\n  `mean(.estimate)`\n              &lt;dbl&gt;\n1              4.13"
  },
  {
    "objectID": "Intro-CV.html#exercise-6-comparing-models",
    "href": "Intro-CV.html#exercise-6-comparing-models",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 6: Comparing Models",
    "text": "EXERCISE 6: Comparing Models\nFill in the table below to summarize the in-sample and 10-fold CV MAE for both models.\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n3.48\n4.13\n\n\nmodel_2\n3.37\n6.28\n\n\n\n\nBased on the in-sample MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on the CV MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on all of these results, which model would you pick?\n\n\nYOUR ANSWER HERE\n\n\nDo the in-sample and CV MAE suggest that model_1 is overfit to our health_data sample data? What about model_2?\n\n\nIt looks like the MAE is roughly similar for when it is measured in sample (3.48) versus when tested on “new” test data(each test fold held out)(4.13). However, model 2 seems overfit becuaes its predictions for new patients data(giving on MAE of 6.28) are much worse than its predictions for patients in out data set."
  },
  {
    "objectID": "Intro-CV.html#exercise-7-loocv",
    "href": "Intro-CV.html#exercise-7-loocv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 7: LOOCV",
    "text": "EXERCISE 7: LOOCV\n\nReconsider model_1. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our health_data sample?\n\n\n# CODE HERE\nmodel_1_loocv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = nrow(health_data)), \n    metrics = metric_set(mae)\n  )\n\n\nmodel_1_loocv %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard    4.30    40   0.981 Preprocessor1_Model1\n\n\n40 is the number of the observations\n\nHow does the LOOCV MAE compare to the 10-fold CV MAE of ___? NOTE: These are just two different approaches to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.\n\n\nANSWER.\n\n\nExplain why we technically don’t need to set.seed() for the LOOCV algorithm.\n\n\nANSWER."
  },
  {
    "objectID": "Lab9_Lasso.html",
    "href": "Lab9_Lasso.html",
    "title": "Lab 9: LASSO",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "href": "Lab9_Lasso.html#build-the-model-for-a-range-of-tuning-parameter-values",
    "title": "Lab 9: LASSO",
    "section": "Build the Model for a Range of Tuning Parameter Values",
    "text": "Build the Model for a Range of Tuning Parameter Values\n\n# STEP 1: LASSO Model Specification\nlasso_spec &lt;- linear_reg() %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_args(mixture = 1, penalty = tune())\n\nSTEP 1 Notes:\n\nWe use the glmnet, not lm, engine to build the LASSO.\nThe glmnet engine requires us to specify some arguments (set_args):\n\nmixture = 1 indicates LASSO. Changing this would run a different regularization algorithm.\npenalty = tune() indicates that we don’t (yet) know an appropriate \\(\\lambda\\) penalty term. We need to tune it.\n\n\nSuppose we want to build a model of response variable y using all possible predictors in a data frame sample_data.\n\n# STEP 2: Variable Recipe\nvariable_recipe &lt;- recipe(height ~ ., data = health_data) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nSTEP 2 Notes:\n\ny ~ . is shorthand for “y as a function of all other variables”\nThe function step_dummy() turns all potentially categorical (sometimes called nominal) predictors into indicator variables (which are unfortunately called “dummy variables” in some circles, hence the terrible name)\n\n\n# STEP 3: Workflow Specification (Model + Recipe)\nlasso_workflow &lt;- workflow() %&gt;% \n  add_recipe(variable_recipe) %&gt;% \n  add_model(lasso_spec)\n\nSTEP 3 Notes:\n\nThe function add_recipe includes our variable_recipe created in Step 2, which is specifying what predictors/response variables we have and what data set those variables belong to\nThe function add_model includes our lasso_spec created in Step 1, which is specifying what kind of model we wish to use (in this case, regression with LASSO)\n\n\n# STEP 4: Estimate Multiple LASSO Models Using a Range of Possible Lambda Values\nset.seed(123)\nlasso_models &lt;- lasso_workflow %&gt;% \n  tune_grid(\n    grid = grid_regular(penalty(range = c(-5, -0.1)), levels = 50),\n    resamples = vfold_cv(health_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\nSTEP 4 Notes:\n\nSince the CV process is random, we need to set.seed(___).\nWe use tune_grid() instead of fit() since we have to build multiple LASSO models, each using a different tuning parameter.\nThe function grid specifies the values of tuning parameter \\(\\lambda\\) that we want to try.\n\npenalty(range = c(___, ___)) specifies a range of \\(\\lambda\\) values we want to try, on the log10 scale.\nYou might start with c(-5, 1), which uses the range \\(\\lambda\\) from 0.00001 (\\(10^(-5)\\)) to 10 (\\(10^1\\)), and adjust from there.\nThe function levels is the number of \\(\\lambda\\) values to try in that range, thus how many LASSO models to build.\n\nThe functionsresamples and metrics indicate that we want to calculate a CV MAE (since mae is in metric_set) for each LASSO model. The number of folds in our cross-validation is given by v."
  },
  {
    "objectID": "Lab9_Lasso.html#tuning-lambda",
    "href": "Lab9_Lasso.html#tuning-lambda",
    "title": "Lab 9: LASSO",
    "section": "Tuning \\(\\lambda\\)",
    "text": "Tuning \\(\\lambda\\)\n\nPlotting CV MAE (Y-Axis) for the LASSO Model for Each \\(\\lambda\\) (X-axis)\n\n# Calculate CV MAE for each LASSO model\nlasso_models %&gt;% collect_metrics()\n\n# Plotting option 1: plot lambda on log10 scale\nautoplot(lasso_models) + scale_x_log10()\n\n# Plotting option 2: plot lambda on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() +\n  xlab(expression(lambda))\n\n# Plotting option 3: CV MAE (y-axis) with error bars (+/- 1 standard error)\n# with lambda (x-axis) on original scale\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.5)\n\n\n# Identify lambda which produced the lowest (\"best\") CV MAE\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\nbest_penalty\n\n# Identify the largest lambda for which the CV MAE is\n# larger but \"roughly as good\" (within one standard error of the lowest)\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\nparsimonious_penalty\n\n\n\nFinalizing the “Best” LASSO Model\n\n# Parameters = final lambda value (best_penalty or parsimonious_penalty)\nfinal_lasso_model &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = ___) %&gt;% \n  fit(data = sample_data)\n\n# Check it out\nfinal_lasso_model %&gt;% tidy()\n\n\n\nUsing Final LASSO Model to Make Predictions\n\nfinal_lasso_model %&gt;% \n  predict(new_data = SOME DATA FRAME W/ OBSERVATIONS ON EACH PREDICTOR)\n\n\n\nVisualizing Shrinkage\nThis code can help us visualize shrinkage by comparing LASSO coefficients under each \\(\\lambda\\).\n\n# Get output for each LASSO\nall_lassos &lt;- final_lasso_model %&gt;% \n  extract_fit_parsnip() %&gt;%\n  pluck(\"fit\")\n\n# Plot coefficient paths as a function of lambda\nplot(all_lassos, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n# Codebook for which variables the numbers correspond to\nrownames(all_lassos$beta)\n\n# For example, what are variables 2 and 4?\nrownames(all_lassos$beta)[c(2,4)]"
  },
  {
    "objectID": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "href": "Lab9_Lasso.html#lasso-least-absolute-shrinkage-and-selection-operator",
    "title": "Lab 9: LASSO",
    "section": "LASSO: least absolute shrinkage and selection operator",
    "text": "LASSO: least absolute shrinkage and selection operator\nIdea\nPenalize a predictor for adding complexity to the model (by penalizing its coefficient). Then track whether the predictor’s contribution to the model (lowering RSS) is enough to offset this penalty.\nCriterion\nIdentify the model coefficients \\(\\hat{\\beta}_1, \\hat{\\beta}_2, ...  \\hat{\\beta}_p\\) that minimize the penalized residual sum of squares:\n\\[SSR + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j\\vert\\]\nwhere\n\nsum of squared residuals (SSR) measures the overall model prediction error\nthe penalty term measures the overall size of the model coefficients\n\\(\\lambda \\ge 0\\) (“lambda”) is a tuning parameter\n\n\nCOMMENT: Picking \\(\\lambda\\)\nWe cannot know the “best” value for \\(\\lambda\\) in advance. This varies from analysis to analysis.\nWe must try a reasonable range of possible values for \\(\\lambda\\). This also varies from analysis to analysis.\nIn general, we have to use trial-and-error to identify a range that is…\n\nwide enough that it doesn’t miss the “best” values for \\(\\lambda\\)\nnarrow enough that it focuses on reasonable values for \\(\\lambda\\)\n\nWe will explore what this means in more detail in one of the exercises."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-1",
    "href": "Lab9_Lasso.html#exercise-1",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 1",
    "text": "EXERCISE 1\nLet’s compare the CV MAEs (y-axis) for our 50 LASSO models which used 50 different \\(\\lambda\\) values (x-axis):\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda))\n\nError in autoplot(lasso_models): object 'lasso_models' not found\n\n\n\nWe told R to use a range of \\(\\lambda\\) from -5 to -0.1 on the log10 scale. Calculate this range on the non-log scale and confirm that it matches the x-axis.\n\nWhat can we observe from this plot?\n\nANSWER."
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-2",
    "href": "Lab9_Lasso.html#exercise-2",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 2",
    "text": "EXERCISE 2\n\nIn the plot above, roughly which value of the \\(\\lambda\\) penalty parameter produces the smallest CV MAE?\n\n\nANSWER. Maybe lambda = 4 for me from the plot? But from running the code chunk below, the “ideal” bambda is about 3.06\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nbest_penalty &lt;- lasso_models %&gt;% \n  select_best(metric = \"mae\")\n\nError in select_best(., metric = \"mae\"): object 'lasso_models' not found\n\nbest_penalty\n\nError in eval(expr, envir, enclos): object 'best_penalty' not found\n\n\n\nSuppose we prefer a parsimonious model.\n\nThe “parsimonious model” is the one with the fewest necessary variables while still maintaining predictive accuracy.\nIn our case, we will define it as the model with the largest possible \\(\\lambda\\) (i.e., biggest penalty for adding new variables) that still has a CV MAE within 1 standard error of the “best” model (the model whose \\(\\lambda\\) gives the lowest CV MAE).\nThe plot below adds error bars to the CV MAE estimates of prediction error (+/- one standard error). Any model with a CV MAE that falls within another model’s error bars is not significantly better or worse at prediction:\n\n# This is copied exactly from the R Code Notes for LASSO Section\nautoplot(lasso_models) + \n  scale_x_continuous() + \n  xlab(expression(lambda)) + \n  geom_errorbar(data = collect_metrics(lasso_models),\n                aes(x = penalty, ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5)\n\nError in autoplot(lasso_models): object 'lasso_models' not found\n\n\nUse this to approximate the largest \\(\\lambda\\), thus the most simple LASSO model, that produces a CV MAE that’s within 1 standard error of the best model (thus is not significantly worse).\n\nANSWER.\n\nCheck your approximation with code.\n\n# This is copied exactly from the R Code Notes for LASSO Section\nparsimonious_penalty &lt;- lasso_models %&gt;% \n  select_by_one_std_err(metric = \"mae\", desc(penalty))\n\nError in select_by_one_std_err(., metric = \"mae\", desc(penalty)): object 'lasso_models' not found\n\nparsimonious_penalty\n\nError in eval(expr, envir, enclos): object 'parsimonious_penalty' not found\n\n\n\nMoving forward, we’ll use the parsimonious LASSO model. Simply report the tuning parameter \\(\\lambda\\) here.\n\n\nANSWER.\n\n\nPicking a Range to Try for \\(\\lambda\\)\nThe range of values we tried for \\(\\lambda\\) had the following nice properties. (If it didn’t, we should adjust our range (make it narrower or wider).\n\nOur range was wide enough.\n\nThe “best” and “parsimonious” \\(\\lambda\\) values were not at the edges of the range, suggesting there aren’t better \\(\\lambda\\) values outside our range.\n\nOur range was narrow enough.\n\nWe didn’t observe any loooooong flat lines in CV MAE. Thus, we narrowed in on the \\(\\lambda\\) values where the “action is happening”, i.e. where changing \\(\\lambda\\) impacts the model.\n\n\n\n\nEXERCISE 3\nModify your previous code to start with \\(\\lambda\\) in the range 10^(-5) to 10^(-0.1) and see what you observe.\n\nANSWER"
  },
  {
    "objectID": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "href": "Lab9_Lasso.html#exercise-4-finalizing-our-lasso-model",
    "title": "Lab 9: LASSO",
    "section": "EXERCISE 4: Finalizing our LASSO model",
    "text": "EXERCISE 4: Finalizing our LASSO model\nLet’s finalize our parsimonious LASSO model:\n\nfinal_lasso &lt;- lasso_workflow %&gt;% \n  finalize_workflow(parameters = parsimonious_penalty) %&gt;% \n  fit(data = health_data)\n\nError in finalize_workflow(., parameters = parsimonious_penalty): object 'lasso_workflow' not found\n\nfinal_lasso %&gt;% \n  tidy()\n\nError in tidy(.): object 'final_lasso' not found\n\n\n\nHow many and which predictors were kept in this model?\n\n\nANSWER.\n\n\nThrough shrinkage, the LASSO coefficients(the \\(\\hat\\beta_i\\)) lose some contextual meaning, so we typically shouldn’t interpret them. Why? THINK: What is the goal of LASSO modeling?\n\n\nANSWER."
  },
  {
    "objectID": "41625.html",
    "href": "41625.html",
    "title": "41625",
    "section": "",
    "text": "election = read_csv(\"Election.csv\")\nhead(election)\n\n# A tibble: 6 × 40\n  state   county   fips trump16 trump16_pct clinton16 clinton16_pct otherpres16\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 Alabama Autauga  1001   18172        72.8      5936         23.8          865\n2 Alabama Baldwin  1003   72883        76.5     18458         19.4         3874\n3 Alabama Barbour  1005    5454        52.1      4871         46.5          144\n4 Alabama Bibb     1007    6738        76.4      1874         21.2          207\n5 Alabama Blount   1009   22859        89.3      2156          8.43         573\n6 Alabama Bullock  1011    1140        24.2      3530         74.9           40\n# ℹ 32 more variables: otherpres16_pct &lt;dbl&gt;, romney12 &lt;dbl&gt;,\n#   romney12_pct &lt;dbl&gt;, obama12 &lt;dbl&gt;, obama12_pct &lt;dbl&gt;, otherpres12 &lt;dbl&gt;,\n#   otherpres12_pct &lt;dbl&gt;, senaterace16 &lt;chr&gt;, houserace16 &lt;chr&gt;,\n#   govrace16 &lt;chr&gt;, total_population &lt;dbl&gt;, cvap &lt;dbl&gt;,\n#   voterturnout16_pct &lt;dbl&gt;, voterturnout12_pct &lt;dbl&gt;,\n#   voterturnout_pct_diff &lt;dbl&gt;, white_pct &lt;dbl&gt;, nonwhite_pct &lt;dbl&gt;,\n#   black_pct &lt;dbl&gt;, hispanic_pct &lt;dbl&gt;, foreignborn_pct &lt;dbl&gt;, …\n\n\nI will describe my final project here:\n\n# Create a scatter plot\nggplot(election, aes(x = female_pct, y = trump16_pct)) +\ngeom_point() +\ngeom_smooth(method = \"lm\")"
  }
]